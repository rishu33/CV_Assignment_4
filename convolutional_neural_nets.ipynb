{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5530fced2150f71fec62adad5bc059ab",
     "grade": false,
     "grade_id": "cell-d9dbc74c08a75dc1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Important**:\n",
    "Follow the instructions below when submitting your attempt.\n",
    "Submissions not following these instructions will not be graded.\n",
    "\n",
    "1. Submit in **teams of 3 or 4 students**, add their names and matriculation numbers below. Only **one team member should upload** the solutions.\n",
    "2. **Use jupyter notebook**. Other notebook-editing software (e.g. jupyter-lab, pycharm) might corrupt the notebook files and could have issues with displaying matplotlib interactively.\n",
    "3. **Do not remove, modify or duplicate** any given cells, except those in which you need to fill in your implementation. You can add new cells in order to present additional texts or plots.\n",
    "4. **Restart the kernel and re-run the whole notebook** once before submission. After this step, the cell id should be incremental from top to bottom, and all plots should be displayed.\n",
    "5. **Submit only the `.ipynb` files**, do not upload archives (zip, rar, tar, etc.), images or datasets.\n",
    "6. **Do not change the filenames** of the `.ipynb` files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Team members (names and matriculation numbers):\n",
    "* \n",
    "* \n",
    "* \n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "998274597e857a7baae7a8e41f51bcd8",
     "grade": false,
     "grade_id": "cell-67e1fcdd86afffc8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Convolutional Neural Networks\n",
    "\n",
    "In this exercise you will be introduced to some practical aspects of deep learning in computer vision, including constructing a deep neural network and training it via gradient descent to tackle image classification. The PyTorch-framework will be used for this.\n",
    "\n",
    "We will tackle **image classification** through deep learning methods, in particular we will look at\n",
    "\n",
    "* Softmax regression with stochastic gradient descent and Adam\n",
    "* Multilayer perceptrons with tanh and ReLU\n",
    "* Data augmentation\n",
    "* A basic convolutional net\n",
    "* BatchNorm, striding, global average pooling\n",
    "* Residual networks\n",
    "* Learning rate decay\n",
    "\n",
    "\n",
    "### Install PyTorch\n",
    "\n",
    "Use `conda install -y pytorch torchvision cpuonly -c pytorch` to install PyTorch without GPU support (make sure that the correct conda environment is active). You may also install a GPU-build if you have a compatible GPU, see https://pytorch.org/get-started/locally/. Using a GPU will make the training several times faster, but since not all of you may have one, we have tried to scale this exercise with a CPU in mind.\n",
    "\n",
    "### TensorBoard Plotting\n",
    "\n",
    "TensorBoard is a web-based tool for drawing pretty plots of quantities we care about during training, such as the loss. Install it using `conda install -y tensorboard=2.8 -c conda-forge`.\n",
    "\n",
    "We need to choose a folder where these values will be stored (\"logdir\"). Start the TensorBoard server by executing e.g. `tensorboard --logdir tensorboard_logs` after you've activated your conda environment. If you change the logdir, also adjust it in the cell below.\n",
    "\n",
    "You can view the graphs by visiting http://localhost:6006/#custom_scalars in your browser (6006 is the default port).\n",
    "Make sure that \"Custom Scalars\" is selected at the top (not \"Scalars\") because this will give a better visualization.\n",
    "At first there will be nothing to plot, so it will be empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_ROOT = 'tensorboard_logs'\n",
    "USE_GPU = False  # Set to True if you have installed tensorflow for GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4e89c7101aa8040b705db9cd9df1d578",
     "grade": false,
     "grade_id": "cell-61c4af0db2f8c0b1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "<!-- Run this cell to add heading letters per subtask (like a, b, c) -->\n",
    "<style>\n",
    "body {counter-reset: section;}\n",
    "h2:before {counter-increment: section;\n",
    "           content: counter(section, lower-alpha) \") \";}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "20ace9163af2ca105e7e1709be8a721e",
     "grade": false,
     "grade_id": "cell-1073cd4f1b08084c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Just an image plotting function\n",
    "def plot_multiple(images, titles=None, colormap='gray',\n",
    "                  max_columns=np.inf, imwidth=2, imheight=2, share_axes=False):\n",
    "    \"\"\"\n",
    "    Plot multiple images as subplots on a grid. Images must be channel-first\n",
    "    and between [0, 1].\n",
    "    \"\"\"\n",
    "    images = [np.transpose(im, (1, 2, 0)) for im in images]\n",
    "    if titles is None:\n",
    "        titles = [''] * len(images)\n",
    "    assert len(images) == len(titles)\n",
    "    n_images = len(images)\n",
    "    n_cols = min(max_columns, n_images)\n",
    "    n_rows = int(np.ceil(n_images / n_cols))\n",
    "    fig, axes = plt.subplots(\n",
    "        n_rows, n_cols, figsize=(n_cols * imwidth, n_rows * imheight),\n",
    "        squeeze=False, sharex=share_axes, sharey=share_axes)\n",
    "\n",
    "    axes = axes.flat\n",
    "    # Hide subplots without content\n",
    "    for ax in axes[n_images:]:\n",
    "        ax.axis('off')\n",
    "        \n",
    "    if not isinstance(colormap, (list,tuple)):\n",
    "        colormaps = [colormap]*n_images\n",
    "    else:\n",
    "        colormaps = colormap\n",
    "\n",
    "    for ax, image, title, cmap in zip(axes, images, titles, colormaps):\n",
    "        ax.imshow(image, cmap=cmap)\n",
    "        ax.set_title(title)\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "        \n",
    "    fig.tight_layout()\n",
    "    \n",
    "def visualize_dataset(dataset, n_samples=24, max_columns=6):\n",
    "    xs, ys = list(zip(*[dataset[i] for i in range(n_samples)]))\n",
    "    plot_multiple([x / 2 + 0.5 for x in xs], [labels[i] for i in ys], max_columns=max_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b2ad2b3393aa21ed9038ff278d04f6c3",
     "grade": false,
     "grade_id": "cell-258c52e8e84d0db9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Dataset Preparation\n",
    "We are going to tackle the classic image classification task using the **CIFAR-10 dataset**, containing 60,000 32x32 RGB images of 10 different classes (50,000 for training and 10,000 for testing). \n",
    "\n",
    "![image.png](cifar.png)\n",
    "\n",
    "The dataset is automatically downloaded if you run the next cell.\n",
    "It will take some time and might produce quite some output with the default Jupyter settings, but you can ignore this.\n",
    "After the dataset was downloaded once, this will not happen anymore.\n",
    "You may read more about the dataset at https://www.cs.toronto.edu/~kriz/cifar.html.\n",
    "\n",
    "Note, that for historical reasons PyTorch uses a \"channels-first\"-format, i.e. the order of the dimensions for an image is `NCHW` (batch-channels-height-width), not `NHWC` (batch-height-width-channels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2580e9b4f35d3f8934f869e159a45b24",
     "grade": false,
     "grade_id": "cell-31d81f8c56fa930d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "normalize_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(0.5, 0.5)\n",
    "])\n",
    "\n",
    "train_data = CIFAR10(root='cifar10/train/', train=True, download=True, transform=normalize_transform)\n",
    "test_data = CIFAR10(root='cifar10/test/', train=False, download=True, transform=normalize_transform)\n",
    "labels = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "x, y = train_data[0]  # get an example from the dataset\n",
    "print(f'\\nShape of an image: {x.shape}.')\n",
    "visualize_dataset(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f3b7f1fd44649770a44332cd5b8d4fc6",
     "grade": false,
     "grade_id": "cell-e030eef482140b24",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Softmax Regression\n",
    "\n",
    "Before considering convolutional neural networks, let us start with a simpler classifier called softmax regression (a.k.a. multinomial logistic regression). Note that even though the name contains \"regression\", this is a classification model.\n",
    "\n",
    "Softmax regression can be understood as a single-layer neural network. We first flatten our input image to a long vector $\\mathbf{x}$, consisting of $32\\cdot 32\\cdot 3 = 3072$ values. Then we predict class probabilities $\\hat{\\mathbf{y}}$ through a fully-connected layer with softmax activation:\n",
    "\n",
    "$$\n",
    "\\mathbf{z} = W \\mathbf{x} + \\mathbf{b} \\\\\n",
    "\\hat{y}_c = \\operatorname{softmax}(\\mathbf{z})_c = \\frac{\\exp{z_c}}{\\sum_{\\tilde{c}=1}^{10} \\exp{z_{\\tilde{c}}}}\n",
    "$$\n",
    "\n",
    "Here $z_c$ denotes the $c$th component of the vector $\\mathbf{z}$, called the vector of **logits**.\n",
    "The weights $W$ and biases $\\mathbf{b}$ will be learned during training.\n",
    "\n",
    "### Training\n",
    "\n",
    "We train the model by minimizing a **loss function** averaged over the training data. As we are tackling a classification problem, the **cross-entropy** is a suitable loss function:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{CE}(\\mathbf{y}, \\hat{\\mathbf{y}}; W, \\mathbf{b}) = - \\sum_{c=1}^{10} y_c \\log{\\hat{y}_c}\n",
    "$$\n",
    "\n",
    "Note that in the above notation the ground-truth $\\mathbf{y}$ is a so-called **one-hot vector**, containing a single 1 component, while the remaining components \n",
    "are zeros. The model's predicted $\\hat{\\mathbf{y}}$ is a vector which also sums to one, but whose components all take continuous values in the range $(0, 1)$.\n",
    "\n",
    "We minimize the loss by **stochastic gradient descent** (SGD). That is, we repeatedly sample mini-batches from the training data and update the parameters (weights and biases) towards the direction of the steepest decrease of the loss averaged over the mini-batch. For example, the weight $w_{ij}$ (an element of the matrix $W$) is updated according to:\n",
    "\n",
    "$$\n",
    "w_{ij}^{(t+1)} = w_{ij}^{(t)} - \\eta \\cdot \\frac{\\partial \\mathcal{L}_{CE}} {\\partial w_{ij}},\n",
    "$$\n",
    "\n",
    "with $\\eta$ being the learning rate.\n",
    "\n",
    "----\n",
    "\n",
    "To build such a model in PyTorch, we take a `Sequential` module which accepts layers that are applied sequentially. Here we have two layers: `Flatten` converts the image into a long vector and `Linear` is a synonym for fully-connected layer. At the end we would expect a `Softmax` layer, but this has to be omitted since the PyTorch-implementation of cross-entropy already performs the softmax implicitly. If you check the equations above, you see why this is advantageous: the cross-entropy applies a $\\log$ to the $\\exp$ of the softmax, which cancel out. By doing both at once, we avoid unnecessary computations and reduce the risk of numerical issues.\n",
    "\n",
    "We further define the function `train_classifier` which performs the training process. These are the main steps:\n",
    "\n",
    "* Instantiate a `DataLoader` for each dataset split which shuffles the order of the data and creates minibatches.\n",
    "* Instantiate the `CrossEntropyLoss`. Note, that this loss implicitely computes the softmax.\n",
    "* Iterate over the epochs:\n",
    "  * Do the training loop:\n",
    "    * Pass the image batch through the model.\n",
    "    * Compute the loss.\n",
    "    * Backpropagate the loss (this computes the gradients for each parameter).\n",
    "    * Do an optimizer step (this updates the model parameters).\n",
    "    * Clear the computed gradients.\n",
    "  * Do a testing loop.\n",
    "  \n",
    "During this process, several statistics (loss/accuracy for train/test) are computed, printed and written to TensorBoard. The function also supports specifying a learning rate scheduler, which we will use at the end of this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "86e1a1a7ff3bdd9229b445e5ce63197b",
     "grade": false,
     "grade_id": "cell-7348264e521960a4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def train_classifier(model,\n",
    "                     opt,\n",
    "                     logdir,\n",
    "                     train_data=train_data,\n",
    "                     test_data=test_data,\n",
    "                     batch_size=128,\n",
    "                     n_epochs=50,\n",
    "                     lr_scheduler=None\n",
    "                    ):\n",
    "    \n",
    "    writer = SummaryWriter(f\"{LOG_ROOT}/{logdir}-{time.strftime('%y%m%d_%H%M%S')}\")\n",
    "    layout = {\n",
    "        'Losses': {'losses': ['Multiline', ['loss/train', 'loss/test']]},\n",
    "        'Accuracy': {'accuracy': ['Multiline', ['accuracy/train', 'accuracy/test']]}\n",
    "    }\n",
    "    writer.add_custom_scalars(layout)\n",
    "    \n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=6)\n",
    "    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True, num_workers=6)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        sample_count = 0\n",
    "        loss_sum = 0\n",
    "        correct = 0\n",
    "        n_batches = len(train_loader)\n",
    "        model.train()\n",
    "        for i, (xs, ys) in enumerate(train_loader):\n",
    "            if USE_GPU:\n",
    "                xs = xs.cuda()\n",
    "                ys = ys.cuda()\n",
    "            out = model(xs)\n",
    "            loss = criterion(out, ys)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "            \n",
    "            loss_sum += loss.item() * xs.shape[0]\n",
    "            _, pred = torch.max(out, 1)\n",
    "            correct += (pred == ys).sum().item()\n",
    "            sample_count += xs.shape[0]\n",
    "            print(f'Train epoch {epoch+1}, step {i+1}/{n_batches}', end='    \\r')\n",
    "\n",
    "        train_loss = loss_sum / sample_count\n",
    "        train_accuracy = correct / sample_count\n",
    "        \n",
    "        with torch.no_grad():  # do not store gradients during testing, decreases memory consumption\n",
    "            sample_count = 0\n",
    "            loss_sum = 0\n",
    "            correct = 0\n",
    "            n_batches = len(test_loader)\n",
    "            model.eval()\n",
    "            for i, (xs, ys) in enumerate(test_loader):\n",
    "                if USE_GPU:\n",
    "                    xs = xs.cuda()\n",
    "                    ys = ys.cuda()\n",
    "                out = model(xs)\n",
    "                loss = criterion(out, ys)\n",
    "                loss_sum += loss.item() * xs.shape[0]\n",
    "                _, pred = torch.max(out, 1)\n",
    "                correct += (pred == ys).sum().item()\n",
    "                sample_count += xs.shape[0]\n",
    "                print(f'Test epoch {epoch+1}, step {i+1}/{n_batches}', end='    \\r')\n",
    "                \n",
    "            test_loss = loss_sum / sample_count\n",
    "            test_accuracy = correct / sample_count\n",
    "        \n",
    "        writer.add_scalar('loss/train', train_loss, epoch+1)\n",
    "        writer.add_scalar('accuracy/train', train_accuracy, epoch+1)\n",
    "        writer.add_scalar('loss/test', test_loss, epoch+1)\n",
    "        writer.add_scalar('accuracy/test', test_accuracy, epoch+1)\n",
    "            \n",
    "        if lr_scheduler is not None:\n",
    "            lr_scheduler.step()\n",
    "            writer.add_scalar('lr', opt.param_groups[0]['lr'], epoch+1)\n",
    "            \n",
    "        print(\n",
    "            f'Epoch {epoch+1} | train loss: {train_loss:.3f}, train accuracy: {train_accuracy:.3f}, ' + \\\n",
    "            f'test loss: {test_loss:.3f}, test accuracy: {test_accuracy:.3f}, ' + \\\n",
    "            f'time: {str(datetime.timedelta(seconds=int(time.time()-start)))}'\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9243c0e4085a758104a48565615ac9b1",
     "grade": false,
     "grade_id": "cell-5c34f08b3b61750a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "softmax_regression = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(in_features=3072, out_features=10)\n",
    ")\n",
    "if USE_GPU:\n",
    "    softmax_regression.cuda()\n",
    "\n",
    "opt = optim.SGD(softmax_regression.parameters(), lr=1e-2)\n",
    "train_classifier(softmax_regression, opt, 'softmax_regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "89fdbd0a9d66df917d6bd8900b5a3abb",
     "grade": false,
     "grade_id": "cell-6cc309c333f4c13a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "(Jupyter Notebook Tip: you can collapse or hide the output by clicking or double clicking the area directly to the left of the output.)\n",
    "\n",
    "You can check the how the loss and accuracy (= proportion of correctly predicted classes) change over the course of training in TensorBoard. What do you observe?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3002df91fe8b11ec0de07b1f11b5c193",
     "grade": true,
     "grade_id": "cell-268ce60b8704b0ab",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "*POINTS: 0*\n",
    "\n",
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "276690c0a6d3c5fe97135c605cd2a096",
     "grade": false,
     "grade_id": "cell-ec4a31aa3b860db5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Adam Optimizer\n",
    "\n",
    "There has been a lot of research on improving on the simple stochastic gradient descent algorithm we used above. One of the most popular variants is called **Adam** (https://arxiv.org/abs/1412.6980, \"adaptive moment estimation\"). Its learning rate usually requires less precise tuning, and something in the range of $(10^{-4},10^{-3})$ often works well in practice. Intuitively, this is because the algorithm automatically adapts the learning rate for each weight depending on the gradients.\n",
    "\n",
    "You can run it as follows. The difference is not large for such a simple model, but it makes a bigger difference for larger networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5a3b87f369822389e015ce63bbe365ce",
     "grade": false,
     "grade_id": "cell-645a91d07f56cfbc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "softmax_regression_adam = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(in_features=3072, out_features=10)\n",
    ")\n",
    "if USE_GPU:\n",
    "    softmax_regression_adam.cuda()\n",
    "    \n",
    "opt = optim.Adam(softmax_regression_adam.parameters(), lr=2e-4)\n",
    "train_classifier(softmax_regression_adam, opt, 'softmax_regression_adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fed1f04ef3bda8ba546d6b0333cd5e9a",
     "grade": false,
     "grade_id": "cell-f97936a20e0833a4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Interpreting the Learned Weights\n",
    "\n",
    "Multiplication by the weights $W$ can be interpreted as computing responses to correlation templates per image class.\n",
    "\n",
    "That means, we can reshape the weight array $W$ to a obtain \"template images\".\n",
    "\n",
    "Perform this reshaping to visualize the resulting templates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aeb608dec5afffe0c72f6fb024c1cbf2",
     "grade": true,
     "grade_id": "cell-6b90988a89171165",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# POINTS: 2\n",
    "\n",
    "W = softmax_regression[1].weight.data\n",
    "\n",
    "print(W.shape)\n",
    "# Use W to create the `templates` variable with dimensions [10 (class count), 3 (rgb), height, width]\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# We normalize the templates for visualization\n",
    "mini = templates.min()\n",
    "maxi = templates.max()\n",
    "rescaled_templates = (templates - mini) / (maxi - mini)\n",
    "plot_multiple(rescaled_templates.cpu(), labels, max_columns=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6e83938015417f67a724ea13175c0b97",
     "grade": false,
     "grade_id": "cell-a48193761ec01321",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Do they look as you would expect?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d7af613881542e9a8f107883548d42d4",
     "grade": true,
     "grade_id": "cell-82b5467968214e70",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "*POINTS: 0*\n",
    "\n",
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "08ff8c8811f820077e3eb2f50eb5b483",
     "grade": false,
     "grade_id": "cell-53dcad4765f2ab5b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Multi-Layer Perceptron\n",
    "\n",
    "Softmax regression has a big limitation: the decision surface between any two classes (i.e. the part of the input space where the classification decision changes from one class to another) is a simple hyperplane (\"flat\").\n",
    "\n",
    "The **multi-layer perceptron** (MLP) is a neural network model with additional layer(s) between the input and the logits (so-called hidden layers), with nonlinear activation functions. Why are activation functions needed?\n",
    "\n",
    "Before the current generation of neural networks, the **hyperbolic tangent** (tanh) function used to be the preferred activation function in the hidden layers of MLPs. It is sigmoid shaped and has a range of $(-1,1)$. We can create such a network in PyTorch as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5f93d0513ee124c5efa20791409a1570",
     "grade": false,
     "grade_id": "cell-1d3f759cb639c7c8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "tanh_mlp = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(in_features=3072, out_features=512),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(in_features=512, out_features=10)\n",
    ")\n",
    "if USE_GPU:\n",
    "    tanh_mlp.cuda()\n",
    "    \n",
    "opt = optim.Adam(tanh_mlp.parameters(), lr=2e-4)\n",
    "train_classifier(tanh_mlp, opt, f'tanh_mlp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a85465b2a9af5fe2cd9965924f11de6f",
     "grade": false,
     "grade_id": "cell-2b139794e1c54e5e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Does it obtain better results than the linear model? What do you observe if you compare the accuracy curves for training and testing?\n",
    "\n",
    "How and why does the behaviour of the test loss differ from the test accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "295ddd6574614bb7c5f16135081ce8c6",
     "grade": true,
     "grade_id": "cell-9a16b9da4520bc8b",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "*POINTS: 0*\n",
    "\n",
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7f11654e0fbff346a19a4a529a4ced25",
     "grade": false,
     "grade_id": "cell-16d91ae3e3806a98",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Data Augmentation\n",
    "\n",
    "To avoid this effect, we can use data augmentation. By applying small changes to the training samples, we can virtually increase the size of the dataset such that the model cannot memorize training samples as easily.\n",
    "\n",
    "Have a look at the `torchvision.transforms` module of torchvision and add the following augmentations below:\n",
    "\n",
    "* shift the images randomly horizontally and vertically by up to 10% of the height and width\n",
    "* scale the images randomly between 90% and 110%\n",
    "* mirror the images such that the right and the left side are swapped in 50% of the cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "815c9341c166bbee3a91566fa786701c",
     "grade": true,
     "grade_id": "cell-4158553d60385578",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# POINTS: 4\n",
    "\n",
    "augment_transform = transforms.Compose([\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "])\n",
    "augmented_train_data = CIFAR10(root='cifar10/train/', train=True, download=True,\n",
    "                               transform=transforms.Compose([augment_transform, normalize_transform]))\n",
    "visualize_dataset(augmented_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "af7dfb228ccff015fbd8ef65fcc9c70f",
     "grade": false,
     "grade_id": "cell-17fa5a99f79769c6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "tanh_mlp = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(in_features=3072, out_features=512),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(in_features=512, out_features=10)\n",
    ")\n",
    "if USE_GPU:\n",
    "    tanh_mlp.cuda()\n",
    "    \n",
    "opt = optim.Adam(tanh_mlp.parameters(), lr=2e-4)\n",
    "train_classifier(tanh_mlp, opt, f'tanh_mlp_augmented', train_data=augmented_train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "82d2b5e66c27b52dc8dfde3b09a28e37",
     "grade": false,
     "grade_id": "cell-fa5e3df235209477",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "What do you observe if you train the same model on the augmented data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "07f3bbbe9a7b29b26ebca41b3c381634",
     "grade": true,
     "grade_id": "cell-d1b52278a5587b93",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "*POINTS: 0*\n",
    "\n",
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4168ee52c510e7246b5d5ceb3065af3a",
     "grade": false,
     "grade_id": "cell-52198a15ed79a2ee",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## ReLU\n",
    "\n",
    "The ReLU activation function has become more popular in recent years, especially for deeper nets. Create and train an MLP with the same architecture as above which uses ReLU instead of tanh as the activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c68d9faf1a2d044e7fa06be03870e807",
     "grade": true,
     "grade_id": "cell-209c6c6b658fdc85",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# POINTS: 2\n",
    "\n",
    "relu_mlp = nn.Sequential(\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    ")\n",
    "if USE_GPU:\n",
    "    relu_mlp.cuda()\n",
    "\n",
    "    \n",
    "opt = optim.Adam(relu_mlp.parameters(), lr=2e-4)\n",
    "train_classifier(relu_mlp, opt, 'relu_mlp', train_data=augmented_train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "629c37c2cc924da566c2f47fc2ca7509",
     "grade": false,
     "grade_id": "cell-b2c3bfe1f9ceab2f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Do the results change? What benefit does ReLU have against tanh?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fcb315ede9df6fb36e2478877c483803",
     "grade": true,
     "grade_id": "cell-e96f4ec268fbbe6c",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "*POINTS: 0*\n",
    "\n",
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8da556076d1cf40cec74144684278eb0",
     "grade": false,
     "grade_id": "cell-3c893def9eef0d29",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## A Simple Convolutional Neural Network\n",
    "\n",
    "The previous models did not explicitly make use of the grid structure of the image pixels. Convolutional neural networks do.\n",
    "\n",
    "Instead of reshaping the input image pixels into one long vector, convolutional layers slide small filters across the input, just as with the convolutional filters we saw earlier in the course. In the earlier parts, we looked at convolution on an image with a single channel in case of grayscale images, or channelwise separate convolutions on RGB images.\n",
    "\n",
    "In CNNs, the multiple input channels of a conv layer are not handled independently, but are linearly combined. This means that the weight array has shape `[out_channels, in_channels, kernel_height, kernel_width]` and we perform a weighted sum along the input channel axis. Another difference is the use of a **bias** vector of shape `[out_channels]`, each components gets added to the corresponding output channel.\n",
    "\n",
    "As you already know, convolution is a linear operator, so it is possible to express any convolutional layer as a fully-connected layer.\n",
    "However, the convolutional layer's weight matrix is sparse (has many zeros) compared to a fully-connected (\"linear\") layer because each output only depends on a small number of inputs, namely, those within a small neigborhood. Further, the weight values are shared between the different pixel locations.\n",
    "\n",
    "This tutorial has some great visualisations and explanations on the details of conv layers: https://arxiv.org/abs/1603.07285."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4eaa7ab3a6ffec239b70a6a4b372c0b1",
     "grade": false,
     "grade_id": "cell-d5bd65e1aa6cf240",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Technically, what's called a \"convolutional\" layer is usually implemented as a *cross-correlation* computation. Could there be any advantage in using the actual definition of convolution in these layers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "58e275ffa399da240c5d609182a29849",
     "grade": true,
     "grade_id": "cell-9f6f8235182c9277",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "*POINTS: 0*\n",
    "\n",
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "eca973fcbc3ba58a184233c1304b4b52",
     "grade": false,
     "grade_id": "cell-8861acb4ed9aa147",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Train the following simple CNN model. It may take about 30 minutes on a CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ab5327988589fc187f717c6529f714de",
     "grade": false,
     "grade_id": "cell-d01c8cd05d41bef2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "cnn = nn.Sequential(\n",
    "    nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "    nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(in_features=4096, out_features=10)\n",
    ")\n",
    "if USE_GPU:\n",
    "    cnn.cuda()\n",
    "\n",
    "opt = optim.Adam(cnn.parameters(), lr=1e-3)\n",
    "train_classifier(cnn, opt, 'cnn', train_data=augmented_train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9499e32100d839b47abf3ca34c056e4c",
     "grade": false,
     "grade_id": "cell-ec21497c60752bff",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Does it improve the result? Does it run faster than the MLP?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5822a2c3f1912ec22098888cf844e24f",
     "grade": true,
     "grade_id": "cell-e6010db8bc2020df",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "*POINTS: 0*\n",
    "\n",
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "17176673004e05c8844e010631ad793f",
     "grade": false,
     "grade_id": "cell-2ee4f12368d36c69",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "How many parameters does this model have? How many parameters has the MLP? Show the steps of your computation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "516a0d4f0bf00fa1c6dc54d46636ec54",
     "grade": true,
     "grade_id": "cell-599a5093a914d24d",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "*POINTS: 0*\n",
    "\n",
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "17fb275a2893b83465467e397305d397",
     "grade": false,
     "grade_id": "cell-ae5c926bd88fcedb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Implementing the Forward Pass\n",
    "\n",
    "To confirm your understanding of the main CNN components, implement the forward pass of the convolutional, max pooling and dense layers, plus the relu activation function. For simplicity, assume a fixed filter size of 3x3 for the convolution, with stride 1 and use zero padding, such that the spatial size does not change (called 'same' padding). Implement this in `conv3x3_same`. For max pooling assume a fixed 2x2 pooling size and stride 2 in `maxpool2x2`.\n",
    "\n",
    "To check whether your implementation is correct, we can extract the weights from the PyTorch model we trained above, and feed these weights and an test input to your implementation of the forward pass. If your result disagrees with PyTorch, there is probably a bug somewhere!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "43bc3ea466742b0614a27f4ba75c8d22",
     "grade": true,
     "grade_id": "cell-e8926ce2153b13ca",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# POINTS: 10\n",
    "\n",
    "def conv3x3_same(x, weights, biases):\n",
    "    \"\"\"Convolutional layer with filter size 3x3 and 'same' padding.\n",
    "    `x` is a NumPy array of shape [in_channels, height, width]\n",
    "    `weights` has shape [out_channels, in_channels, kernel_height, kernel_width]\n",
    "    `biases` has shape [out_channels]\n",
    "    Return the output of the 3x3 conv (without activation)\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return result\n",
    "\n",
    "def maxpool2x2(x):\n",
    "    \"\"\"Max pooling with pool size 2x2 and stride 2.\n",
    "    `x` is a numpy array of shape [in_channels, height, width]\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return result\n",
    "\n",
    "def linear(x, weights, biases):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "def relu(x):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "def my_predict_cnn(x, W1, b1, W2, b2, W3, b3):\n",
    "    x = conv3x3_same(x, W1, b1)\n",
    "    x = relu(x)\n",
    "    x = maxpool2x2(x)\n",
    "    x = conv3x3_same(x, W2, b2)\n",
    "    x = relu(x)\n",
    "    x = maxpool2x2(x)\n",
    "    x = x.reshape(-1)\n",
    "    x = linear(x, W3, b3)\n",
    "    return x\n",
    "\n",
    "W1 = cnn[0].weight.data.cpu().numpy()\n",
    "b1 = cnn[0].bias.data.cpu().numpy()\n",
    "W2 = cnn[3].weight.data.cpu().numpy()\n",
    "b2 = cnn[3].bias.data.cpu().numpy()\n",
    "W3 = cnn[7].weight.data.cpu().numpy()\n",
    "b3 = cnn[7].bias.data.cpu().numpy()\n",
    "\n",
    "inp = train_data[0][0]\n",
    "inp_np = inp.numpy()\n",
    "if USE_GPU:\n",
    "    inp = inp.cuda()\n",
    "my_logits = my_predict_cnn(inp_np, W1, b1, W2, b2, W3, b3)\n",
    "pytorch_logits = cnn(inp[np.newaxis])[0]\n",
    "if np.mean((my_logits-pytorch_logits.detach().cpu().numpy())**2) > 1e-5:\n",
    "    print('Something isn\\'t right! PyTorch gives different results than my_predict_cnn!')\n",
    "else:\n",
    "    print('Congratulations, you got correct results!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dec1c39a9a1c02b7281de1fd2622c108",
     "grade": false,
     "grade_id": "cell-959a911a2a175121",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Batch Normalization\n",
    "\n",
    "Batch normalization is a modern technique to improve and speed up the training of deep neural networks (BatchNorm, Ioffe & Szegedy ICML'15, https://arxiv.org/abs/1502.03167). Each feature channel is normalized to have zero mean and unit variance across the spatial and mini-batch axes. To compensate for the lost degrees of freedom, extra scaling and bias parameters are introduced and learned. Mathematically, BatchNorm for a spatial feature map (e.g. the output of conv) can be written as:\n",
    "\n",
    "$$\n",
    "\\mu_d = \\mathbb{E}\\{x_{\\cdot \\cdot d}\\}, \\\\\n",
    "\\sigma_d = \\sqrt{\\operatorname{Var}\\{x_{\\cdot \\cdot d}\\}} \\\\\n",
    "z_{ijd} = \\gamma_d \\cdot \\frac{x_{ijd} - \\mu_d}{\\sigma_d} + \\beta_d,\\\\\n",
    "$$\n",
    "\n",
    "with the expectation and variance taken across both the data samples of the batch and the spatial dimensions.\n",
    "\n",
    "The $\\mu_d$ and $\\sigma_d$ values are computed on the actual mini-batch during training, but at test-time they are fixed, so that the prediction of the final system on a given sample does not depend on other samples in the mini-batch. To obtain the fixed values for test-time use, one needs to maintain moving statistics over the activations during training. This can be a bit tricky to implement from scratch, but luckily this is now implemented in all popular frameworks, including PyTorch.\n",
    "\n",
    "When applying BatchNorm, it is not necessary to use biases in the previous convolutional layer. Why? Use the `bias` argument of `nn.Conv2d` accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "384036a5283192262c7ff77892cec960",
     "grade": true,
     "grade_id": "cell-0178a85974e9358e",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "*POINTS: 0*\n",
    "\n",
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2c7d1c4f4b79114e42807242ca306cf7",
     "grade": false,
     "grade_id": "cell-1a1a3ac922ed7e52",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Create a modified version of the previous model using batch normalization between each convolution and the corresponding activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f839f8a1cf077c9064b5b47c62022aa6",
     "grade": true,
     "grade_id": "cell-83b754b10f9a5f09",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# POINTS: 3\n",
    "\n",
    "cnn_batchnorm = nn.Sequential(\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    ")\n",
    "if USE_GPU:\n",
    "    cnn_batchnorm.cuda()\n",
    "\n",
    "opt = optim.Adam(cnn_batchnorm.parameters(), lr=1e-3)\n",
    "train_classifier(cnn_batchnorm, opt, 'cnn_batchnorm', train_data=augmented_train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a7ac28f2a6faced5ad48faa2f43224f6",
     "grade": false,
     "grade_id": "cell-57707594f666ee77",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Strided Convolutions\n",
    "\n",
    "Max-pooling is a popular technique for reducing the spatial dimensionality\n",
    "of the outputs from conv layers. Another way to reduce dimensionality is striding. For an argument why this may be similarly effective, see [Springenberg et al., ICLRW'15](https://arxiv.org/pdf/1412.6806.pdf).\n",
    "\n",
    "Now create a model using the same architecture as before, with the difference of\n",
    "removing the max-pooling layers and increasing the stride parameter of the conv layers to $2 \\times 2$ in the spatial dimensions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "735a9eb696991a3230e2314330847d97",
     "grade": true,
     "grade_id": "cell-34f5d6a1166b46fa",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# POINTS: 2\n",
    "\n",
    "cnn_strides = nn.Sequential(\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    ")\n",
    "if USE_GPU:\n",
    "    cnn_strides.cuda()\n",
    "\n",
    "opt = optim.Adam(cnn_strides.parameters(), lr=1e-3)\n",
    "train_classifier(cnn_strides, opt, 'cnn_strides', train_data=augmented_train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fb27ebaacf2db8e350735285db940944",
     "grade": false,
     "grade_id": "cell-cbb6e2eeb2dcc66e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "What differences do you notice when training this new network?\n",
    "What is a clear advantage of using strides?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c00f4ff39d8abb65116bdf0a0589d1c7",
     "grade": true,
     "grade_id": "cell-12ee310e8234b8f8",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "*POINTS: 0*\n",
    "\n",
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "10c6a5679d00652a999570b133d4f792",
     "grade": false,
     "grade_id": "cell-bc32808a463893fd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Global Pooling\n",
    "\n",
    "The above network ends in a `Flatten` layer followed by a `Linear` layer, in which the number of weights depends on the input size. This means that testing can only be performed on the exact same image size. Several architectures employ a (spatial) **global average pooling layer** to produce a vector of fixed size describing the whole image.\n",
    "\n",
    "Implement the forward pass of such a layer below. The input `x` has size `[batch_size, channels, height, width]`. The mean must be computed across the last two dimensions, such that the result `pooled` has a size of `[batch_size, channels]`. Note, that you cannot use numpy for this as PyTorch is not able to backpropagate through numpy functions. There are however corresponding PyTorch functions for almost all numpy functions.\n",
    "\n",
    "This layer can now replace the flattening operation from the previous network. However, the units before the average pooling need to have a large enough receptive field, otherwise the model will not work well. Therefore, compared with the previous model, remove the `Flatten` layer and instead add a third Conv-BatchNorm-ReLU combination. Then add `GlobalAvgPool2d` and a final `Linear` layer which returns $10$ values per sample instead of $64$.\n",
    "\n",
    "Train it and see if it reaches similar accuracy to the previous one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "db818f085096735ade425e7442ddc610",
     "grade": true,
     "grade_id": "cell-384e1eaafbd3f3b6",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# POINTS: 4\n",
    "\n",
    "class GlobalAvgPool2d(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        return pooled\n",
    "\n",
    "\n",
    "cnn_global_pool = nn.Sequential(\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    ")\n",
    "if USE_GPU:\n",
    "    cnn_global_pool.cuda()\n",
    "\n",
    "opt = optim.Adam(cnn_global_pool.parameters(), lr=1e-3)\n",
    "train_classifier(cnn_global_pool, opt, 'cnn_global_pool', train_data=augmented_train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ee6de3f7f259be2d205f59714faefb0a",
     "grade": false,
     "grade_id": "cell-527e589864d3a660",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Which network has more parameters, this or the previous one?\n",
    "\n",
    "What is the size of the receptive field of the units in the layer directly before the global average pooling? (Remember: the receptive field of a particular unit (neuron) is the area of the *input image* that can influence the activation of this given unit)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "df0c0d22fa5a21c06cc25bcc63a499c6",
     "grade": true,
     "grade_id": "cell-a36f95947bd5ba86",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "*POINTS: 0*\n",
    "\n",
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bf60b8f446c3347eb7b6509af0b9607b",
     "grade": false,
     "grade_id": "cell-cfc670e8ea147092",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Residual Networks\n",
    "\n",
    "ResNet is a more modern architecture, introduced by He et al. in 2015 (published in 2016: https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf) and is still popular today.\n",
    "\n",
    "It consists of blocks like the following:\n",
    "\n",
    "![ResNet Block](resnet_block.png)\n",
    "\n",
    "Each of these so-called *residual blocks* only have to predict a *residual* (in plain words: the \"rest\", the \"leftover\") that will be added on top of its input.\n",
    "In other words, the block outputs how much each feature needs to be changed in order to enhance the representation compared to the previous block.\n",
    "\n",
    "There are several ways to combine residual blocks into *residual networks* (ResNets). In the following, we consider ResNet-v1, as used for the CIFAR-10 benchmark in the original ResNet paper (it is simpler compared to the full model that they used for the much larger ImageNet benchmark).\n",
    "\n",
    "Section 4.2. of the paper describes this architecture as follows: \"*The first layer is 33 convolutions. Then we use a stack of 6n layers with 33 convolutions on the feature maps of sizes {32, 16, 8} respectively, with 2n layers for each feature map size. The numbers of filters are {16, 32, 64} respectively. The subsampling is performed by convolutions with a stride of 2. The network ends with a global average pooling, a 10-way fully-connected layer, and softmax. [...] When shortcut connections are used, they are connected to the pairs of 33 layers (totally 3n shortcuts). On this dataset we use identity shortcuts in all cases.*\"\n",
    "\n",
    "Further, they use L2 regularization for training (a standard tool to combat overfitting). This penalizes weights with large magnitude by adding an additional term to the cost function, besides the cross-entropy. The overall function to optimize becomes:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{CE} + \\frac{\\lambda}{2} \\sum_{w\\in\\text{weights}} w^2,\n",
    "$$\n",
    "\n",
    "and in this paper $\\lambda=10^{-4}$.\n",
    "\n",
    "Use the explanation above to complete the `layers`-list in the `ResNet`-class below. Note, that the first layer is already added and remember that the final softmax has to be omitted. The `ResNetBlock` already implements the above figure, i.e. contains $2$ convolutions.\n",
    "\n",
    "Weight decay is already added by setting the corresponding attribute of the opimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "059f3c3e5ab6822cbbd7ace3650f979f",
     "grade": true,
     "grade_id": "cell-78512b7752e9d7d2",
     "locked": false,
     "points": 6,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# POINTS: 6\n",
    "\n",
    "class ResNetBlock(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "        self.f = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "        self.activation = nn.ReLU()\n",
    "        # The shortcut connection is just the identity. If feature\n",
    "        # channel counts differ between input and output, zero\n",
    "        # padding is used to match the depths. This is implemented\n",
    "        # by a convolution with the following fixed weight:\n",
    "        self.pad_weight = nn.Parameter(\n",
    "            torch.eye(out_channels, in_channels)[:, :, None, None],\n",
    "            requires_grad=False\n",
    "        )\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        r = self.f(x)\n",
    "        # We apply the padding weight using torch.functional.conv2d\n",
    "        # which allows us to use a custom weight matrix.\n",
    "        x = F.conv2d(x, self.pad_weight, stride=self.stride)\n",
    "        return self.activation(x + r)\n",
    "    \n",
    "\n",
    "class ResNet(torch.nn.Module):\n",
    "    def __init__(self, num_layers=14, in_channels=3, out_features=10):\n",
    "        super().__init__()\n",
    "        if (num_layers - 2) % 6 != 0:\n",
    "            raise ValueError('n_layers should be 6n+2 (eg 20, 32, 44, 56)')\n",
    "        n = (num_layers - 2) // 6\n",
    "        \n",
    "        layers = []\n",
    "        \n",
    "        first_layer = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 16, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        layers.append(first_layer)\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ffd7372a683b0cf2d589e1d59bd49dc1",
     "grade": false,
     "grade_id": "cell-ac38cfcbccbc3e34",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Train ResNet-18 on the CIFAR-10 dataset for 50 epochs. As a rough idea, it will take less than 15 minutes with a good GPU, but on a CPU it can take several hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8176ed445f8204508e3d3d803fd02bbb",
     "grade": false,
     "grade_id": "cell-e3b3f6d8fa93f028",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "resnet = ResNet()\n",
    "if USE_GPU:\n",
    "    resnet.cuda()\n",
    "\n",
    "opt = optim.Adam(resnet.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "train_classifier(resnet, opt, f'resnet', train_data=augmented_train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6d30d6ddca7fb16235c3573be47b0266",
     "grade": false,
     "grade_id": "cell-434819020b48d6b4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Learning Rate Decay\n",
    "\n",
    "Learning rate decay reduces the learning rate as the training progresses. Use the same settings as in the previous experiment, but this time create a `MultiStepLR`-scheduler and decrease the learning rate twice by a factor of 10 each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e40b5ceaf07d7fb7d7b5ac5af7ef340e",
     "grade": true,
     "grade_id": "cell-f2042420b7d15963",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# POINTS: 2\n",
    "\n",
    "resnet_decay = ResNet()\n",
    "if USE_GPU:\n",
    "    resnet_decay.cuda()\n",
    "\n",
    "opt = optim.Adam(resnet_decay.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "train_classifier(resnet_decay, opt, 'resnet_decay', lr_scheduler=scheduler, train_data=augmented_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
